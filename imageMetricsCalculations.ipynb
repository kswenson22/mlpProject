{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.utils import save_image\n",
    "import numpy as np\n",
    "from scipy.linalg import sqrtm\n",
    "from torchvision.models import inception_v3\n",
    "import torchvision.transforms.functional as TF\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import os\n",
    "import multiprocessing\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.utils import save_image\n",
    "import numpy as np\n",
    "from scipy.linalg import sqrtm\n",
    "from torchvision.models import inception_v3\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch.nn.functional as F\n",
    "import multiprocessing\n",
    "import joblib\n",
    "from joblib import load\n",
    "import torchvision.models as models\n",
    "import csv\n",
    "\n",
    "device = torch.device('mps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the generator architecture (CHANGE THIS EVERY TIME)\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv_block = nn.Sequential(\n",
    "            nn.Conv2d(channels, channels, kernel_size=3, stride=1, padding=1),\n",
    "            nn.InstanceNorm2d(channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(channels, channels, kernel_size=3, stride=1, padding=1),\n",
    "            nn.InstanceNorm2d(channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.conv_block(x)\n",
    "    \n",
    "# ARCHITECTURE FOR MONET + BIG CATS\n",
    "    \n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_nc, output_nc):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        # Define the generator model using nn.Sequential\n",
    "        self.model = nn.Sequential(\n",
    "            # Initial convolution block\n",
    "            nn.Conv2d(input_nc, 64, kernel_size=7, stride=2, padding=3, padding_mode='reflect'),\n",
    "            nn.InstanceNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            # Downsampling\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),\n",
    "            nn.InstanceNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1),\n",
    "            nn.InstanceNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            # Residual blocks\n",
    "            ResidualBlock(256),\n",
    "            ResidualBlock(256),\n",
    "            ResidualBlock(256),\n",
    "            ResidualBlock(256),\n",
    "            ResidualBlock(256),\n",
    "            ResidualBlock(256),\n",
    "            \n",
    "            # Upsampling\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.InstanceNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.InstanceNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, output_nc, kernel_size=7, stride=2, padding=3, output_padding=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "# class Generator(nn.Module):\n",
    "#     def __init__(self, input_nc, output_nc):\n",
    "#         super(Generator, self).__init__()\n",
    "\n",
    "#         self.model = nn.Sequential(\n",
    "#             # Encoder layers\n",
    "#             nn.Conv2d(input_nc, 64, kernel_size=4, stride=2, padding=1),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n",
    "#             nn.InstanceNorm2d(128),\n",
    "#             nn.ReLU(),\n",
    "#             # Residual blocks\n",
    "#             ResidualBlock(128),\n",
    "#             ResidualBlock(128),\n",
    "#             ResidualBlock(128),\n",
    "#             ResidualBlock(128),\n",
    "#             ResidualBlock(128),\n",
    "#             ResidualBlock(128),\n",
    "#             # Decoder layers\n",
    "#             nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),\n",
    "#             nn.InstanceNorm2d(64),\n",
    "#             nn.ReLU(),\n",
    "#             nn.ConvTranspose2d(64, output_nc, kernel_size=4, stride=2, padding=1),\n",
    "#             nn.Tanh()\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return self.model(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_image_pair_metrics(real_images, fake_images):\n",
    "    ssim_scores = []\n",
    "    psnr_scores = []\n",
    "\n",
    "    # Move images to CPU and convert to numpy for SSIM and PSNR calculations\n",
    "    real_images = real_images.to('cpu').numpy()\n",
    "    fake_images = fake_images.to('cpu').numpy()\n",
    "\n",
    "    for real_img, fake_img in zip(real_images, fake_images):\n",
    "        # Adjust format for SSIM and PSNR\n",
    "        real_img_np = np.moveaxis(real_img, 0, -1)  # Convert from CxHxW to HxWxC\n",
    "        fake_img_np = np.moveaxis(fake_img, 0, -1)\n",
    "\n",
    "        # Specify win_size if necessary, here it's set as an example and may need to be adjusted\n",
    "        win_size = min(7, real_img_np.shape[0], real_img_np.shape[1])  # Ensure win_size is appropriate\n",
    "\n",
    "        # Calculate and collect metrics\n",
    "        ssim_score = ssim(real_img_np, fake_img_np, data_range=real_img_np.max() - real_img_np.min(), multichannel=True, win_size=win_size, channel_axis=2)\n",
    "        psnr_score = psnr(real_img_np, fake_img_np, data_range=real_img_np.max() - real_img_np.min())\n",
    "\n",
    "        ssim_scores.append(ssim_score)\n",
    "        psnr_scores.append(psnr_score)\n",
    "\n",
    "    return ssim_scores, psnr_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up the correct paths and dataloaders (THIS NEEDS TO BE CHANGED EVERY TIME)\n",
    "epoch = 100\n",
    "model_name = 'photo2monet'\n",
    "checkpoint_path = f'/Users/kieran/Documents/mlpProject/runs/{model_name}/epoch_{epoch}.pth'\n",
    "checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "\n",
    "input_nc = 3  # Number of input channels\n",
    "output_nc = 3  # Number of output channels\n",
    "# Initialize networks\n",
    "G_AB = Generator(input_nc, output_nc).to(device)\n",
    "G_BA = Generator(output_nc, input_nc).to(device)\n",
    "\n",
    "# Load the state dictionaries\n",
    "G_AB.load_state_dict(checkpoint['G_AB_state_dict'])\n",
    "G_BA.load_state_dict(checkpoint['G_BA_state_dict'])\n",
    "\n",
    "# Ensure the models are in evaluation mode\n",
    "G_AB.eval()\n",
    "G_BA.eval()\n",
    "\n",
    "# Prepare your dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "train_dataset_A = ImageFolder(root='/Users/kieran/Documents/mlpProject/zebraHorse/trainA', transform=transform)\n",
    "train_dataset_B = ImageFolder(root='/Users/kieran/Documents/mlpProject/zebraHorse/trainB', transform=transform)\n",
    "test_dataset_A = ImageFolder(root='/Users/kieran/Documents/mlpProject/bigCats/test/TIGER', transform=transform)\n",
    "test_dataset_B = ImageFolder(root='/Users/kieran/Documents/mlpProject/bigCats/test/TIGER', transform=transform)\n",
    "\n",
    "train_dataloader_A = DataLoader(train_dataset_A, batch_size=8, shuffle=True)\n",
    "train_dataloader_B = DataLoader(train_dataset_B, batch_size=8, shuffle=True)\n",
    "test_dataloader_A = DataLoader(train_dataset_A, batch_size=8, shuffle=True)\n",
    "test_dataloader_B = DataLoader(train_dataset_B, batch_size=8, shuffle=True)\n",
    "\n",
    "# Generate and save/display images\n",
    "output_dir = f'/Users/kieran/Documents/mlpProject/{model_name}/imageMetrics/{epoch}'\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for Train:\n",
      "  ssim_AB: -0.1271\n",
      "  psnr_AB: 8.7768\n",
      "  ssim_BA: -0.1744\n",
      "  psnr_BA: 8.5131\n",
      "Results for Test:\n",
      "  ssim_AB: -0.1271\n",
      "  psnr_AB: 8.7768\n",
      "  ssim_BA: -0.1744\n",
      "  psnr_BA: 8.5131\n"
     ]
    }
   ],
   "source": [
    "def calculate_metrics_for_pair(G_AB, G_BA, device, dataloader_A, dataloader_B, save_dir, label):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    # Initialize metrics dictionary for the pair of dataloaders\n",
    "    metrics = {'ssim_AB': [], 'psnr_AB': [], 'ssim_BA': [], 'psnr_BA': []}\n",
    "\n",
    "    for _, (real_A_batch, _) in enumerate(dataloader_A):\n",
    "        real_A_batch = real_A_batch.to(device)\n",
    "        with torch.no_grad():\n",
    "            fake_B_batch = G_AB(real_A_batch)\n",
    "        ssim_AB, psnr_AB = calculate_image_pair_metrics(real_A_batch, fake_B_batch)\n",
    "        metrics['ssim_AB'].extend(ssim_AB)\n",
    "        metrics['psnr_AB'].extend(psnr_AB)\n",
    "\n",
    "    for _, (real_B_batch, _) in enumerate(dataloader_B):\n",
    "        real_B_batch = real_B_batch.to(device)\n",
    "        with torch.no_grad():\n",
    "            fake_A_batch = G_BA(real_B_batch)\n",
    "        ssim_BA, psnr_BA = calculate_image_pair_metrics(real_B_batch, fake_A_batch)\n",
    "        metrics['ssim_BA'].extend(ssim_BA)\n",
    "        metrics['psnr_BA'].extend(psnr_BA)\n",
    "    \n",
    "    # Print the average metrics for the given pair of dataloaders\n",
    "    print(f\"Results for {label}:\")\n",
    "    for metric_name, scores in metrics.items():\n",
    "        if scores:\n",
    "            print(f\"  {metric_name}: {np.mean(scores):.4f}\")\n",
    "        else:\n",
    "            print(f\"  {metric_name}: No Data\")\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "label_train = 'Train'\n",
    "label_test = 'Test'\n",
    "save_dir_train = os.path.join(output_dir, label_train)\n",
    "save_dir_test = os.path.join(output_dir, label_test)\n",
    "\n",
    "# Calculate and print metrics for the training datasets\n",
    "calculate_metrics_for_pair(G_AB, G_BA, device, train_dataloader_A, train_dataloader_B, save_dir_train, label_train)\n",
    "\n",
    "# Calculate and print metrics for the testing datasets\n",
    "calculate_metrics_for_pair(G_AB, G_BA, device, test_dataloader_A, test_dataloader_B, save_dir_test, label_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kieran/Documents/mlpProject/pytorch_env/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/kieran/Documents/mlpProject/pytorch_env/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=Inception_V3_Weights.IMAGENET1K_V1`. You can also use `weights=Inception_V3_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average FID for Train A: 7.846375299196117e+55\n",
      "Average FID for Train B: -5.88468378721426e+55\n",
      "Average FID for Test A: -1.5652521419868583e+56\n",
      "Average FID for Test B: -8.979469499653194e+51\n"
     ]
    }
   ],
   "source": [
    "from torchvision.transforms import Compose, Resize, Normalize, ToTensor, ToPILImage\n",
    "from torchvision import models\n",
    "import numpy as np\n",
    "from scipy.linalg import sqrtm\n",
    "import certifi\n",
    "os.environ['SSL_CERT_FILE'] = certifi.where()\n",
    "from torchvision.models import inception_v3\n",
    "\n",
    "def calculate_kid_for_dataloaders(G_AB, device, dataloader_A, dataloader_B, label, num_batches=10):\n",
    "    # vgg16 = models.vgg16(pretrained=True)\n",
    "    # model_features = vgg16.features[:31]\n",
    "    # for param in model_features.parameters():\n",
    "    #     param.requires_grad = False\n",
    "    # model_features.eval()\n",
    "    # print(\"Model features loaded\")\n",
    "\n",
    "    inception = inception_v3(pretrained=True, transform_input=False).to(device)\n",
    "    \n",
    "    # Define preprocessing transformation for Inception V3\n",
    "    preprocess = Compose([\n",
    "        ToPILImage(),  # Convert tensors to PIL Images\n",
    "        Resize((299, 299)),  # Resize the image for Inception V3\n",
    "        ToTensor(),  # Convert images to Tensor\n",
    "        Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  # Normalize for Inception V3\n",
    "    ])\n",
    "\n",
    "    kid_A = []  # To store KID scores for each batch\n",
    "    kid_B = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, ((real_A_batch, _), (real_B_batch, _)) in enumerate(zip(dataloader_A, dataloader_B)):\n",
    "            if i >= num_batches:  # Break after processing num_batches\n",
    "                break\n",
    "            # Apply preprocessing to real images\n",
    "            real_A_batch = torch.stack([preprocess(image) for image in real_A_batch]).to(device)\n",
    "            real_B_batch = torch.stack([preprocess(image) for image in real_B_batch]).to(device)\n",
    "            # Generate fake images\n",
    "            fake_A_batch = G_BA(real_B_batch)\n",
    "            fake_B_batch = G_AB(real_A_batch)\n",
    "            # Apply preprocessing to fake images - ensure they are detached from the graph\n",
    "            fake_A_batch = torch.stack([preprocess(image.detach()) for image in fake_A_batch]).to(device)\n",
    "            fake_B_batch = torch.stack([preprocess(image.detach()) for image in fake_B_batch]).to(device)\n",
    "\n",
    "            # Assuming aux_logits=True, extract the main output explicitly\n",
    "            real_features_A = inception(real_A_batch)[0].detach().cpu().numpy()\n",
    "            real_features_B = inception(real_B_batch)[0].detach().cpu().numpy()\n",
    "            fake_features_A = inception(fake_A_batch)[0].detach().cpu().numpy()\n",
    "            fake_features_B = inception(fake_B_batch)[0].detach().cpu().numpy()\n",
    "\n",
    "            # Calculate FID for the batch\n",
    "            kid_A1 = calculate_fid_from_features(real_features_A, fake_features_A)\n",
    "            kid_A.append(kid_A1)\n",
    "\n",
    "            kid_B1 = calculate_fid_from_features(real_features_B, fake_features_B)\n",
    "            kid_B.append(kid_B1)\n",
    "\n",
    "    print(f\"Average FID for {label} A: {np.mean(kid_A)}\")\n",
    "    print(f\"Average FID for {label} B: {np.mean(kid_B)}\")\n",
    "\n",
    "def calculate_fid_from_features(real_features, fake_features):\n",
    "    mu1, sigma1 = real_features.mean(axis=0), np.cov(real_features, rowvar=False)\n",
    "    mu2, sigma2 = fake_features.mean(axis=0), np.cov(fake_features, rowvar=False)\n",
    "    ssdiff = np.sum((mu1 - mu2) ** 2.0)\n",
    "    covmean = sqrtm(sigma1.dot(sigma2))\n",
    "    if np.iscomplexobj(covmean):\n",
    "        covmean = covmean.real\n",
    "    fid = ssdiff + np.trace(sigma1 + sigma2 - 2.0 * covmean)\n",
    "    return fid\n",
    "\n",
    "calculate_kid_for_dataloaders(G_AB, device, train_dataloader_A, train_dataloader_B, label_train)\n",
    "calculate_kid_for_dataloaders(G_AB, device, test_dataloader_A, test_dataloader_B, label_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
