{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.utils import save_image\n",
    "import numpy as np\n",
    "from scipy.linalg import sqrtm\n",
    "from torchvision.models import inception_v3\n",
    "import torchvision.transforms.functional as TF\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import os\n",
    "import multiprocessing\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.utils import save_image\n",
    "import numpy as np\n",
    "from scipy.linalg import sqrtm\n",
    "from torchvision.models import inception_v3\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch.nn.functional as F\n",
    "import multiprocessing\n",
    "import joblib\n",
    "from joblib import load\n",
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_nc, output_nc):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            # Encoder layers\n",
    "            nn.Conv2d(input_nc, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.InstanceNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            # Residual blocks\n",
    "            ResidualBlock(128),\n",
    "            ResidualBlock(128),\n",
    "            ResidualBlock(128),\n",
    "            ResidualBlock(128),\n",
    "            ResidualBlock(128),\n",
    "            ResidualBlock(128),\n",
    "            # Decoder layers\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.InstanceNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, output_nc, kernel_size=4, stride=2, padding=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv_block = nn.Sequential(\n",
    "            nn.Conv2d(channels, channels, kernel_size=3, stride=1, padding=1),\n",
    "            nn.InstanceNorm2d(channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(channels, channels, kernel_size=3, stride=1, padding=1),\n",
    "            nn.InstanceNorm2d(channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.conv_block(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated images have been saved to /Users/kieran/Documents/mlpProject/0319_horses2zebras_1e4_batch8/generated_images_train_epoch_100\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('mps')\n",
    "\n",
    "# path to save checkpoint CHANGE EPOCH NOT WHOLE PATH\n",
    "epoch = 100\n",
    "checkpoint_path = f'/Users/kieran/Documents/mlpProject/0319_horses2zebras_1e4_batch8/epoch_100.pth'\n",
    "checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "\n",
    "input_nc = 3  # Number of input channels\n",
    "output_nc = 3  # Number of output channels\n",
    "# Initialize networks\n",
    "G_AB = Generator(input_nc, output_nc).to(device)\n",
    "G_BA = Generator(output_nc, input_nc).to(device)\n",
    "\n",
    "# Load the state dictionaries\n",
    "G_AB.load_state_dict(checkpoint['G_AB_state_dict'])\n",
    "G_BA.load_state_dict(checkpoint['G_BA_state_dict'])\n",
    "\n",
    "# Ensure the models are in evaluation mode\n",
    "G_AB.eval()\n",
    "G_BA.eval()\n",
    "\n",
    "# Prepare your dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "dataset_A = ImageFolder(root='/Users/kieran/Documents/mlpProject/zebraHorse/trainA', transform=transform)\n",
    "dataset_B = ImageFolder(root='/Users/kieran/Documents/mlpProject/zebraHorse/trainB', transform=transform)\n",
    "\n",
    "dataloader_A = DataLoader(dataset_A, batch_size=1, shuffle=False)\n",
    "dataloader_B = DataLoader(dataset_B, batch_size=1, shuffle=False)\n",
    "\n",
    "# Generate and save/display images\n",
    "output_dir = f'/Users/kieran/Documents/mlpProject/0319_horses2zebras_1e4_batch8/generated_images_train_epoch_{epoch}'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, (real_A, _) in enumerate(dataloader_A):\n",
    "        real_A = real_A.to(device)\n",
    "        fake_B = G_AB(real_A)\n",
    "        save_image(fake_B, os.path.join(output_dir, f'fake_B_{i}.png'))\n",
    "        if i == 1000:  # Save/display 10 images for demonstration\n",
    "            break\n",
    "\n",
    "    for i, (real_B, _) in enumerate(dataloader_B):\n",
    "        real_B = real_B.to(device)\n",
    "        fake_A = G_BA(real_B)\n",
    "        save_image(fake_A, os.path.join(output_dir, f'fake_A_{i}.png'))\n",
    "        if i == 1000:\n",
    "            break\n",
    "\n",
    "print(\"Generated images have been saved to\", output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated images have been saved to /Users/kieran/Documents/mlpProject/0313_50features_128/generated_images_train_epoch_100\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('mps')\n",
    "\n",
    "# path to save checkpoint CHANGE EPOCH NOT WHOLE PATH\n",
    "epoch = 100\n",
    "checkpoint_path = f'/Users/kieran/Documents/mlpProject/0313_50features_128/epoch_100.pth'\n",
    "checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "\n",
    "input_nc = 3  # Number of input channels\n",
    "output_nc = 3  # Number of output channels\n",
    "# Initialize networks\n",
    "G_AB = Generator(input_nc, output_nc).to(device)\n",
    "G_BA = Generator(output_nc, input_nc).to(device)\n",
    "\n",
    "# Load the state dictionaries\n",
    "G_AB.load_state_dict(checkpoint['G_AB_state_dict'])\n",
    "G_BA.load_state_dict(checkpoint['G_BA_state_dict'])\n",
    "\n",
    "# Ensure the models are in evaluation mode\n",
    "G_AB.eval()\n",
    "G_BA.eval()\n",
    "\n",
    "# Prepare your dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "dataset_A = ImageFolder(root='/Users/kieran/Documents/mlpProject/zebraHorse/trainA', transform=transform)\n",
    "dataset_B = ImageFolder(root='/Users/kieran/Documents/mlpProject/zebraHorse/trainB', transform=transform)\n",
    "\n",
    "dataloader_A = DataLoader(dataset_A, batch_size=1, shuffle=False)\n",
    "dataloader_B = DataLoader(dataset_B, batch_size=1, shuffle=False)\n",
    "\n",
    "# Generate and save/display images\n",
    "output_dir = f'/Users/kieran/Documents/mlpProject/0313_50features_128/generated_images_train_epoch_{epoch}'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, (real_A, _) in enumerate(dataloader_A):\n",
    "        real_A = real_A.to(device)\n",
    "        fake_B = G_AB(real_A)\n",
    "        save_image(fake_B, os.path.join(output_dir, f'fake_B_{i}.png'))\n",
    "        if i == 1000:  # Save/display 10 images for demonstration\n",
    "            break\n",
    "\n",
    "    for i, (real_B, _) in enumerate(dataloader_B):\n",
    "        real_B = real_B.to(device)\n",
    "        fake_A = G_BA(real_B)\n",
    "        save_image(fake_A, os.path.join(output_dir, f'fake_A_{i}.png'))\n",
    "        if i == 1000:\n",
    "            break\n",
    "\n",
    "print(\"Generated images have been saved to\", output_dir)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
